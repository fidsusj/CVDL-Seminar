\section{Conclusion} \label{conclusion}

Back to the question of \textit{RepVGG: Making VGG-style ConvNets great again?} one concludes that RepVGG convinces by providing a simple to implement VGG-style model with a well-achieved speed-accuracy trade-off by utilizing a structural re-parameterization method. It therefore clearly differentiates in its approach and goal-setting from other approaches like DiracNet \cite{SergeyZagoruyko.2018}, ABCs \cite{XiaohanDing.2019} or special initialization methods \cite{LechaoXiao.2018, OyebadeOyedotun.2020}. 

RepVGG also gives a novel point of view into the design, training and deployment of CNNs with its re-parameterization method as a bridge to enable a separation into a training- and an inference-time architecture. Also considering the concrete deployment infrastructure and adapting the design of the model accordingly (3x3 convolutional layers to be optimized with the Winograd convolutional algorithm \cite{AndrewLavin.2015}) is not to be neglected as a scientific contribution. 

What remains is a plain VGG-like CNN model with noticeable speed gains and the first of its kind achieving an over 80\% accuracy mark on ImageNet \cite{JiaDeng.2009} as a plain CNN model. Therefore RepVGG makes a very important contribution to this specific field of research. The thesis to make VGG-style ConvNets great again fits the results achieved in the original paper. 

%TODO numbers or written numbers?