\section{Introduction}
% Problem statement, motivation, main idea

VGG, Inception, ResNet, DenseNet, Xception, ResNeXt, Efficientnet, RegNet, the history of research in convolutional neural networks (CNNs) is an ancient discipline in computer vision. Over the years such CNNs became more and more accurate and efficient, but also more complicated to implement and to understand as many architecture-specific ideas and components got introduced. Such architectures do not even need to be manually designed anymore as neural architecture search methods give the opportunity to learn optimal architectural designs right away. Old VGG-style plain CNNs containing only 3x3 convolutional, pooling and ReLU activation layers seem to almost be completely eradicated. 

Besides all the benefits of enabling the training of deeper and more accurate models such architectures also have a few downsides. Multi-branch models are not only difficult to implement and to customize, they also do not manage to achieve a decent speed-accuracy trade-off anymore as the multi-branch architectures introduce underloaded memory utilization, synchronization overheads, high memory access costs (MACs) and therefore reduced inference speed. Plain VGG-style models do not own such inherent speed bottlenecks, but are difficult to train to decent depths because of vanishing gradient problems. They also cannot be used as an implicit ensemble of multiple shallower models as ResNet-like architectures can. 

RepVGG therefore introduces a re-parameterization method that helps to transform parameters of a ResNet-like training-time architecture to a VGG-like inference-time architecture using simple linear algebra. The resulting model can thus be trained until reasonable accuracy by increased depth and an implicit ensemble-like setup during training and still achieves far higher speed during inference compared to current multi-branch models. The few types of operators needed for the VGG-like inference-time architecture also help to integrate more computing units onto the chip, which can furthermore individually be optimized on hardware-level giving additional speed gains. Having a plain feed-forward architecture during inference also makes the final model more memory-efficient in the end. 

The goal of RepVGG is it therefore to provide a simple and efficient VGG-style plain CNN during inference obtained by applying a structural re-parameterization onto a trained ResNet-like multi-branch model. RepVGG will be evaluated both for image classification on ImageNet as well as for semantic segmentation on Cityscapes. 

Before the re-parameterization method and architecture of RepVGG will be introduced, an extensive examination of the fundamentals and related work will be given. Many of the models introduced in this section will be important for major arguments, comparisons and the context of later parts of this paper. 
 