\section{Introduction}
% Problem statement, motivation, main idea

VGG, Inception, ResNet, DenseNet, Xception, ResNeXt, Efficientnet, RegNet, the history of research in convolutional neural networks (CNNs) is an ancient discipline in computer vision. Over the years such CNNs became more and more accurate and efficient, but also more complicated to implement and to understand as many architecture-specific ideas and components got introduced. Such architectures do not even need to be manually designed anymore as neural architecture search methods give the opportunity to learn optimal architectural designs right away. Old VGG-style plain CNNs containing only 3x3 convolutional layers, pooling layers and ReLU activation seem to almost be completely eradicated. 

Besides of all the benefits of enabling the training of deeper and more accurate models such architectures also have a few downsides. Such multi-branch models are not only difficult to implement and customize, they also do not manage to achieve a decent speed-accuracy trade-off anymore as the multi-branch architectures introduce underloaded memory utilization, synchronization overheads, high memory access costs (MACs) and therefore reduced inference speed. Plain VGG-style models do not own such inherent speed bottlenecks, but are difficult to train to decent depths because of the vanishing gradient problem. They also cannot be used as an implicit ensemble of multiple shallower models as ResNet-like architectures can. 

RepVGG therefore introduced a re-parameterization method that helps to transform parameters of a ResNet-like training-time architecture to a VGG-like inference-time architecture using simple linear algebra. The resulting model can therefore be trained until reasonable accuracy by increased depth and an implicit ensemble-like setup during training and still achieves far higher speed during inference compared to current multi-branch models. The few types of operators needed for the VGG-like inference-time architecture also help to integrate more computing units onto the chip, which can furthermore individually be optimized on hardware-level giving additional speed gains. Having a plain feed-forward architecture during inference also makes the final model more memory-efficient in the end. 

The goal of RepVGG is it therefore to provide a simple and efficient VGG-style plain CNN during inference obtained by applying a structural re-parameterization onto a trained ResNet-like multi-branch model used for training. RepVGG will be evaluated both for image classification on ImageNet as well as for semantic segmentation in Cityscapes. 
 