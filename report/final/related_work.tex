\section{Related Work}

The VGG architecture was introduced in \cite{KarenSimonyan.2014}. One of its key finings was to prefer deep CNNs (16-19 weight layers) with small receptive fields induced by using small kernels over shallow CNNs with bigger receptive fields. Therefore a configuration of 3x3 kernels with stride 1 were used. This not only helps to strengthen the discriminative character of the network as the non-linear activation function (ReLU) is applied more often but also keeps the number of parameters to train lower. To increase the non-linearity without affecting the related receptive fields even 1x1 kernels were considered in more deeper architectures. Only by using simple convolutional, max-pooling and fully connected layers at the end of the network, VGG achieved a 24.4 top-1 validation error during ILSVRC-2014 (single net performance). \cite{KarenSimonyan.2014}

Inception \cite{ChristianSzegedy.2014}.


- structural re-parameterization technique
- ResNet-50,
- ResNet-101
- Inception
- DenseNet
- EfficientNet
- RegNet
- automatic, or manual architecture search
- search compound scaling strategy